from numpy.core.fromnumeric import nonzero
import pytest

import os
from pathlib import Path
import shutil
import sys
import hashlib
from typing import Dict, Any
import re

import subprocess
import json

from scripts.installer.utils.purge_folders import delete_pycache_folders

# TODO: June 21/2025 -- This is a hack to get the test to run.
root = "/home/deeplearning/cx-field-tools-installer"
tfvars_path = f"{root}/terraform.tfvars"
tfvars_backup_path = f"{root}/terraform.tfvars.backup"
test_tfvars_path = f"{root}/tests/datafiles/terraform.tfvars"

# Cache infrastructure for `terraform plan` results.
_plan_cache: Dict[str, Any] = {}
_cache_dir = f"{root}/tests/.plan_cache"

"""
EXPLANATION
=======================================
Originally tried using [tftest](https://pypi.org/project/tftest/) package to test but this became complicated and unwieldy. Instead, simplified
testing loop to:

1. Test in the project directory.
2. Take a backup of the existing `terraform.tfvars` file.
3. Create a new `terraform.tfvars` file for testing purposes (_sourced from `tests/datafiles/generate_core_data.sh`).
4. Provide override values to test fixtures (which will generate a new `override.auto.tfvars` file in the project root).
    This file supercedes the same keys defined in the `terraform.tfvars` file.
5. Run the tests:
    1. For each fixture, run `terraform plan` based on the test tvars and override file. Results as cached to speed up n+1 test runs.
    2. Execute tests tied to that fixture.
    3. Repeat.
6. Purge the test tfvars and override file.
7.Restore the original `terraform.tfvars` file when testing is complete.
"""


## ------------------------------------------------------------------------------------
## Cache Utility Functions
## ------------------------------------------------------------------------------------
def get_cache_key(override_data: str, qualifier: str = "") -> str:
    """Generate SHA-256 hash of override data and tfvars content for cache key."""
    normalized_override = override_data.strip()
    normalized_qualifier = qualifier.strip()

    # Read tfvars file content and include in cache key
    try:
        with open(tfvars_path, "r") as f:
            tfvars_content = f.read().strip()
    except FileNotFoundError as e:
        print(f"An error occurred: {e}")
        sys.exit(1)

    # Combine override data and tfvars content for cache key
    combined_content = f"{normalized_override}\n---TFVARS---\n{normalized_qualifier}\n---QUALIFIER---\n{tfvars_content}"
    return hashlib.sha256(combined_content.encode("utf-8")).hexdigest()[:16]


def strip_overide_whitespace(override_data: str) -> str:
    """Strip whitespace from override data.
    # Remove intermediate whitespace (extra space makes same keys hash to different values).
    # Convert multiple sapces to single space (ASSUMPTION: Python tabs insert spaces!)
    # NOTE: Need to keep `\n` to not break HCL formatting expectations.
    """
    return "\n".join(re.sub(r"\s+", " ", line) for line in override_data.splitlines())


def read_json(file_path: str) -> dict:
    """Read a JSON plan file."""
    with open(file_path, "r") as f:
        return json.load(f)


def read_file(file_path: str) -> str:
    """Read a file."""
    with open(file_path, "r") as f:
        return f.read()


def write_file(file_path: str, content: str) -> None:
    """Write content to a file."""
    with open(file_path, "w") as f:
        f.write(content)


def run_terraform_plan(qualifier: str) -> bytes:
    """Run terraform plan.
    Override file generated by prepare_plan; core tfvars file generated by fixture.
    """
    print(f"Purging existing tfplan and tfplan.json.")
    Path(f"{root}/tfplan").unlink(missing_ok=True)
    Path(f"{root}/tfplan.json").unlink(missing_ok=True)

    print("Regenerating tfplan.")
    command = f"terraform plan {qualifier} -out=tfplan"
    plan = subprocess.run(
        command.split(" "),
        check=True,
        stdout=subprocess.DEVNULL,  # > /dev/null
        stderr=subprocess.STDOUT,  # 2>&1
    ).stdout

    return plan


def convert_plan_to_json() -> dict[str, Any]:
    print("Regenerating tfplan.json.")
    raw = subprocess.run(
        ["terraform", "show", "-json", "tfplan"],
        check=True,
        capture_output=True,
        text=True,
    ).stdout

    # Path(f"{root}/tfplan.json").write_text(json.dumps(json.loads(raw), indent=2))
    return json.loads(raw)


def run_terraform_apply(qualifier: str = "", auto_approve: bool = True) -> None:
    """Run terraform plan.
    Override file generated by prepare_plan; core tfvars file generated by fixture.
    """
    if qualifier == "":
        print(f"Applying tfplan.")
        command = f"terraform apply {'--auto-approve' if auto_approve else ''} tfplan"
    else:
        # Should not normally need this, but adding just in case.
        print(f"Using {qualifier=} instead of tfplan.")
        command = (
            f"terraform apply {'--auto-approve' if auto_approve else ''} {qualifier}"
        )

    subprocess.run(
        command.split(" "),
        check=True,
        stdout=subprocess.DEVNULL,  # > /dev/null
        stderr=subprocess.STDOUT,  # 2>&1
    )


## ------------------------------------------------------------------------------------
## Helpers
## ------------------------------------------------------------------------------------
def prepare_plan(
    override_data: str, qualifier: str = "", use_cache: bool = True
) -> dict:
    """Generate override.auto.tfvars and run terraform plan with caching.

    Args:
        override_data: Terraform variable overrides
        qualifier: Additional modifier to add to cache key (e.g '-target=null_resource.my_resource')
        use_cache: Whether to use cached results (default: True)

    Returns:
        Terraform plan JSON data
    """

    override_data = strip_overide_whitespace(override_data)
    cache_key = get_cache_key(override_data, qualifier)

    cached_plan_file = f"{_cache_dir}/plan_{cache_key}"

    cache_file = f"{_cache_dir}/plan_{cache_key}.json"

    override_file = f"{root}/override.auto.tfvars"
    tfplan_file = f"{root}/tfplan"
    tfplan_json_file = f"{root}/tfplan.json"

    # Return plan from file if possible. If not, create new cache file and dump terraform plan results into it.
    if use_cache and os.path.exists(cached_plan_file):
        print(f"Loading cached plan from disk: {cache_key}")
        shutil.copy(cached_plan_file, tfplan_file)
        # plan_json = convert_plan_to_json()
        # write_file(tfplan_json_file, json.dumps(plan_json, indent=2))
        # return read_json(cache_file)
    else:
        print(f"Generating new plan for key: {cache_key}")
        os.makedirs(_cache_dir, exist_ok=True)
        write_file(override_file, override_data)
        plan = run_terraform_plan(qualifier)
        shutil.copy(tfplan_file, cached_plan_file)

    plan_json = convert_plan_to_json()
    write_file(tfplan_json_file, json.dumps(plan, indent=2))
    return read_json(f"{root}/tfplan.json")


def parse_key_value_file(file_path: str) -> Dict[str, str]:
    """Parse a file containing KEY=VALUE pairs.

    Args:
        file_path: Path to the file to parse

    Returns:
        Dictionary containing key-value pairs
    """
    result = {}

    try:
        with open(file_path, "r") as f:
            for line in f:
                line = line.strip()
                if line and "=" in line:
                    key, value = line.split("=", 1)
                    result[key.strip()] = value.strip()
    except FileNotFoundError:
        print(f"File not found: {file_path}")
    except Exception as e:
        print(f"Error parsing file {file_path}: {e}")

    return result
